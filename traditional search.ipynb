{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f984bdf9",
   "metadata": {},
   "source": [
    "# RememberKanji: traditional method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7500a82",
   "metadata": {},
   "source": [
    "Creates mnemonic sentences by sampling sentences from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "04a0bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "829edf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c32e1",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "5ce9be71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:amount of kanji: 13108\n"
     ]
    }
   ],
   "source": [
    "## kanji data\n",
    "with open('data/kanji.json', encoding='utf8') as f:\n",
    "    kanji_data = json.load(f, encoding='utf8')\n",
    "    \n",
    "logging.info(f\"amount of kanji: {len(kanji_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "80bd9154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:amount of sentences: 13650\n"
     ]
    }
   ],
   "source": [
    "## sentences\n",
    "directory = './data/more_sentences/data'\n",
    "\n",
    "string_sen = \"\"\n",
    "for filename in os.listdir(directory):\n",
    "    path = os.path.join(directory, filename)\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, errors='ignore') as f:\n",
    "            string_sen =  string_sen + f.read()\n",
    "\n",
    "# remove all not-sentences\n",
    "string_sen = string_sen.replace(\"### abstract ###\", \"\")\n",
    "string_sen = string_sen.replace(\"### introduction ###\", \"\")\n",
    "string_sen = string_sen.replace(\"CITATION\", \"\")\n",
    "string_sen = string_sen.replace(\" ,\", \"\")\n",
    "\n",
    "# split into sentences\n",
    "sentences = string_sen.split(\".\")\n",
    "\n",
    "# bring back split delimiter (period)\n",
    "sentences = [sentences[i].rstrip(' ') + \".\" for i in range(len(sentences))]\n",
    "\n",
    "logging.info(f\"amount of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "2a19d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:amount of sentences: 15159\n"
     ]
    }
   ],
   "source": [
    "## sentences\n",
    "with open('data/stories.json') as f:\n",
    "    sentences_data = json.load(f)\n",
    "\n",
    "for story in sentences_data:\n",
    "    for s in story['story'].split('.'):\n",
    "        sentences.append(s + '.')\n",
    "\n",
    "logging.info(f\"amount of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "75157e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:amount of sentences: 39046\n"
     ]
    }
   ],
   "source": [
    "## sentences\n",
    "with open('data/sentences.json') as f:\n",
    "    sentences_data = json.load(f)\n",
    "\n",
    "for s in sentences_data:\n",
    "    for question in s['questions']:\n",
    "        for context in question['context']:\n",
    "            sentences.append(context['text'])\n",
    "\n",
    "logging.info(f\"amount of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04475721",
   "metadata": {},
   "source": [
    "## execute task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "03e6fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## gets set of keywords from kanji character\n",
    "def get_keywords(character):\n",
    "    \n",
    "    # check if character is present in dataset\n",
    "    if character in kanji_data.keys():\n",
    "        char_properties = kanji_data[character]\n",
    "        \n",
    "        # check if radicals are available\n",
    "        if char_properties['wk_radicals'] != None:\n",
    "            \n",
    "            # add meaning as keyword\n",
    "            keywords = [char_properties['wk_meanings'][0].lower()]\n",
    "            \n",
    "            # add radicals as keywords\n",
    "            [keywords.append(rad.lower()) for rad in char_properties['wk_radicals']]\n",
    "            logging.info(f\"Keywords: {keywords}\")\n",
    "            return keywords\n",
    "        else:\n",
    "            logging.info(\"Radicals not available for this character. Try another one.\")\n",
    "            return []\n",
    "    else:\n",
    "        logging.info(\"Character not available. Try another one.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "81b07f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a character: è¨³\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Keywords: ['translation', 'say', 'shrimp']\n"
     ]
    }
   ],
   "source": [
    "## get keywords from given kanji\n",
    "keywords = get_keywords(input(\"Insert a character: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "4b1069fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "Finally, intrinsic disorder offers an elegant mechanism of regulation through post-translational modifications for many cellular processes.\n",
      "Included keywords: ['translation']\n"
     ]
    }
   ],
   "source": [
    "## check for sentences\n",
    "possible_sen = [sentences]\n",
    "included_keywords = []\n",
    "final_result = \"No sentence found.\"\n",
    "\n",
    "# find sentences with keywords\n",
    "for i in range(len(keywords)):\n",
    "    matching_sen = list(filter(lambda s: keywords[i] in s, possible_sen[i]))\n",
    "    possible_sen.append(matching_sen)\n",
    "    if len(matching_sen):\n",
    "        final_result = matching_sen[0]\n",
    "        included_keywords.append(keywords[i])\n",
    "\n",
    "\n",
    "print(f\"Sentence: {final_result}\")\n",
    "print(f\"Included keywords: {included_keywords}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu2] *",
   "language": "python",
   "name": "conda-env-tf-gpu2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
